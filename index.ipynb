{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import \\\n",
    "    regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import re\n",
    "from collections import OrderedDict, defaultdict, Counter\n",
    "import itertools\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_colwidth = 150\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in the woods...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now in the building across the street</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our area...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "5   8     NaN      NaN   \n",
       "6  10     NaN      NaN   \n",
       "7  13     NaN      NaN   \n",
       "8  14     NaN      NaN   \n",
       "9  15     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "5                         #RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires   \n",
       "6                                        #flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas   \n",
       "7                                                                            I'm on top of the hill and I can see a fire in the woods...   \n",
       "8                                                        There's an emergency evacuation happening now in the building across the street   \n",
       "9                                                                                   I'm afraid that the tornado is coming to our area...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/disaster_tweets/train.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword and location columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location column doesn't seem to have usable information. In some cases it's nonsense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of some of the location values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['World Wide!!',\n",
       " 'Paranaque City',\n",
       " 'Live On Webcam',\n",
       " 'milky way',\n",
       " 'GREENSBORO,NORTH CAROLINA',\n",
       " 'England.',\n",
       " 'Sheffield Township, Ohio',\n",
       " 'India',\n",
       " 'Barbados',\n",
       " 'Anaheim']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sample of some of the location values')\n",
    "df['location'].unique()[7:17].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(nan, 61),\n",
       " ('fatalities', 45),\n",
       " ('armageddon', 42),\n",
       " ('deluge', 42),\n",
       " ('body%20bags', 41),\n",
       " ('damage', 41),\n",
       " ('harm', 41),\n",
       " ('sinking', 41),\n",
       " ('collided', 40),\n",
       " ('evacuate', 40),\n",
       " ('fear', 40),\n",
       " ('outbreak', 40),\n",
       " ('siren', 40),\n",
       " ('twister', 40),\n",
       " ('windstorm', 40),\n",
       " ('collision', 39),\n",
       " ('derailment', 39),\n",
       " ('earthquake', 39),\n",
       " ('explosion', 39),\n",
       " ('famine', 39)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['keyword']).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df[['text']]\n",
    "target = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Target\n",
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n",
      "\n",
      "0    0.57034\n",
      "1    0.42966\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Distribution of Target')\n",
    "print\n",
    "print(target.value_counts())\n",
    "print()\n",
    "print(target.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tweets, target, test_size=.25, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and tokenizing the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a copy of untouched tweets\n",
    "X_train_tweets_unprocessed = X_train.copy()['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-cbb6d635a30d>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['text'] = X_train['text'].apply(lambda x: x.lower())\n"
     ]
    }
   ],
   "source": [
    "X_train['text'] = X_train['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>@masochisticmage + catastrophe! it caused people to get reckless and the bottom line is that at least three of your friends will have +</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>#nochilllukehammings\\nim screaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7589</th>\n",
       "      <td>omg earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>it's never a good sign when you pull up to work &amp;amp; there's five ambulances &amp;amp; a fire truck in the bay. wompppp at least it's friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>my mic and controllers aren't working one second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4569</th>\n",
       "      <td>my baby girls car wreak this afternoon thank god no serious injuries and she was wearing her seatbelt!!!... http://t.co/njqv45nds2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6371</th>\n",
       "      <td>look at the previous battles. citizens were committing suicide so to not be under american control. the bomb was the only way. @nbcnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4648</th>\n",
       "      <td>@mistresspip i'm amazed you have not been inundated mistress.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>maj muzzamil pilot offr of mi-17 crashed near mansehra today. http://t.co/kl4r1ccwct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>christian attacked by muslims at the temple mount after waving israeli flag via pamela geller - ... http://t.co/f5miuhqaby</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text\n",
       "1489    @masochisticmage + catastrophe! it caused people to get reckless and the bottom line is that at least three of your friends will have +\n",
       "5973                                                                                                         #nochilllukehammings\\nim screaming\n",
       "7589                                                                                                                             omg earthquake\n",
       "3788  it's never a good sign when you pull up to work &amp; there's five ambulances &amp; a fire truck in the bay. wompppp at least it's friday\n",
       "825                                                                                            my mic and controllers aren't working one second\n",
       "4569         my baby girls car wreak this afternoon thank god no serious injuries and she was wearing her seatbelt!!!... http://t.co/njqv45nds2\n",
       "6371    look at the previous battles. citizens were committing suicide so to not be under american control. the bomb was the only way. @nbcnews\n",
       "4648                                                                              @mistresspip i'm amazed you have not been inundated mistress.\n",
       "1812                                                       maj muzzamil pilot offr of mi-17 crashed near mansehra today. http://t.co/kl4r1ccwct\n",
       "492                  christian attacked by muslims at the temple mount after waving israeli flag via pamela geller - ... http://t.co/f5miuhqaby"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve if tweet has a URL. Will use this later.\n",
    "\n",
    "def binary_url(text):\n",
    "    \"\"\"\n",
    "    Returns 1 if a string contains a URL, else returns 0.\n",
    "    \"\"\"\n",
    "    search = re.search(pattern=r'http\\S+', string=text)\n",
    "    return int(bool(search))\n",
    "\n",
    "has_url_Series_train = X_train['text'].apply(binary_url).rename('has_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-d605797eee16>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['text'] = X_train['text'].apply(\n"
     ]
    }
   ],
   "source": [
    "X_train['text'] = X_train['text'].apply(\n",
    "    lambda x: re.sub(pattern=r'http\\S+', repl='', string=x)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"such activities of govt can't derail us from our aim &amp; we still remain peaceful and unite for #freesikhpoliticalprisnors &amp; @bapusuratsingh\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tweet = X_train['text'].loc[2372]\n",
    "example_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6073                  rt twit_san_diego 'possible sinkhole disrupts trolley service: a depression in a portion of asphalt in downtown saû_ \n",
       "2078                                            @kg4vaal lmaov.v hard the 'ny' is the the new trend babalmao...welcome to nyozi kwaaaaa#dead\n",
       "3513                      #eyewitness media is actively embraced by #uk audiences. read the report by @emhub on the impact of #ugc in news: \n",
       "2520                                                                fear and panic in the air i want to be free from desolation and despair!\n",
       "2405    @jozerphine literally just look that up! yeah derailed at smithsonian so everythign is shut down from federal center sw to mcpherson\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['text'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = r\"[a-zA-Z]+'?[a-zA-Z]+\"\n",
    "# Pattern: Any alphanumeric word with at least two characters, including up to one apostrophy\n",
    "\n",
    "tokenizer = RegexpTokenizer(token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['such',\n",
       " 'activities',\n",
       " 'of',\n",
       " 'govt',\n",
       " \"can't\",\n",
       " 'derail',\n",
       " 'us',\n",
       " 'from',\n",
       " 'our',\n",
       " 'aim',\n",
       " 'amp',\n",
       " 'we',\n",
       " 'still',\n",
       " 'remain',\n",
       " 'peaceful',\n",
       " 'and',\n",
       " 'unite',\n",
       " 'for',\n",
       " 'freesikhpoliticalprisnors',\n",
       " 'amp',\n",
       " 'bapusuratsingh']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(example_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems to work\n",
    "# But \"amp\" is an artifact of \"&amp;\" which is the HTML entity for \"&\"\n",
    "# Replace it with \"and\" in original text\n",
    "# Then tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-f7ec564c15b2>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['text'] = X_train['text'].apply(lambda x: re.sub(\"&amp;\", \"and\", x))\n"
     ]
    }
   ],
   "source": [
    "X_train['text'] = X_train['text'].apply(lambda x: re.sub(\"&amp;\", \"and\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-1675f3322f1d>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)\n"
     ]
    }
   ],
   "source": [
    "X_train['text_tokenized'] = X_train['text'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "# Get comprehensive list\n",
    "# Combine stopwords from two libraries: NLTK and SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_spacy = list(nlp.Defaults.stop_words)\n",
    "sw_nltk = stopwords.words('english')\n",
    "stopword_list = list(set(sw_spacy + sw_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-968b6c5a6f45>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['text_tokenized_no_sw'] = \\\n"
     ]
    }
   ],
   "source": [
    "X_train['text_tokenized_no_sw'] = \\\n",
    "    X_train['text_tokenized'].apply(\n",
    "        lambda x: [w for w in x if not w in stopword_list]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 264),\n",
       " ('fire', 200),\n",
       " (\"i'm\", 184),\n",
       " ('new', 176),\n",
       " ('news', 164),\n",
       " ('people', 151),\n",
       " ('video', 119),\n",
       " ('disaster', 115),\n",
       " ('police', 110),\n",
       " ('emergency', 108),\n",
       " ('time', 101),\n",
       " ('body', 95),\n",
       " ('suicide', 90),\n",
       " ('california', 90),\n",
       " ('storm', 90),\n",
       " ('burning', 89),\n",
       " ('rt', 88),\n",
       " ('crash', 87),\n",
       " ('world', 84),\n",
       " ('man', 83)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(X_train['text_tokenized_no_sw'].explode()).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"i'm\" should also be considered a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list.extend([\"i'm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-968b6c5a6f45>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['text_tokenized_no_sw'] = \\\n"
     ]
    }
   ],
   "source": [
    "X_train['text_tokenized_no_sw'] = \\\n",
    "    X_train['text_tokenized'].apply(\n",
    "        lambda x: [w for w in x if not w in stopword_list]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 264),\n",
       " ('fire', 200),\n",
       " ('new', 176),\n",
       " ('news', 164),\n",
       " ('people', 151),\n",
       " ('video', 119),\n",
       " ('disaster', 115),\n",
       " ('police', 110),\n",
       " ('emergency', 108),\n",
       " ('time', 101),\n",
       " ('body', 95),\n",
       " ('suicide', 90),\n",
       " ('california', 90),\n",
       " ('storm', 90),\n",
       " ('burning', 89),\n",
       " ('rt', 88),\n",
       " ('crash', 87),\n",
       " ('world', 84),\n",
       " ('man', 83),\n",
       " ('bomb', 82)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20_no_sw = FreqDist(X_train['text_tokenized_no_sw'].explode()).most_common(20)\n",
    "top_20_no_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_tokenized_no_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>@masochisticmage + catastrophe! it caused people to get reckless and the bottom line is that at least three of your friends will have +</td>\n",
       "      <td>[masochisticmage, catastrophe, it, caused, people, to, get, reckless, and, the, bottom, line, is, that, at, least, three, of, your, friends, will,...</td>\n",
       "      <td>[masochisticmage, catastrophe, caused, people, reckless, line, friends]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>#nochilllukehammings\\nim screaming</td>\n",
       "      <td>[nochilllukehammings, im, screaming]</td>\n",
       "      <td>[nochilllukehammings, im, screaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7589</th>\n",
       "      <td>omg earthquake</td>\n",
       "      <td>[omg, earthquake]</td>\n",
       "      <td>[omg, earthquake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>it's never a good sign when you pull up to work and there's five ambulances and a fire truck in the bay. wompppp at least it's friday</td>\n",
       "      <td>[it's, never, good, sign, when, you, pull, up, to, work, and, there's, five, ambulances, and, fire, truck, in, the, bay, wompppp, at, least, it's,...</td>\n",
       "      <td>[good, sign, pull, work, there's, ambulances, fire, truck, bay, wompppp, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>my mic and controllers aren't working one second</td>\n",
       "      <td>[my, mic, and, controllers, aren't, working, one, second]</td>\n",
       "      <td>[mic, controllers, working, second]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         text  \\\n",
       "1489  @masochisticmage + catastrophe! it caused people to get reckless and the bottom line is that at least three of your friends will have +   \n",
       "5973                                                                                                       #nochilllukehammings\\nim screaming   \n",
       "7589                                                                                                                           omg earthquake   \n",
       "3788    it's never a good sign when you pull up to work and there's five ambulances and a fire truck in the bay. wompppp at least it's friday   \n",
       "825                                                                                          my mic and controllers aren't working one second   \n",
       "\n",
       "                                                                                                                                             text_tokenized  \\\n",
       "1489  [masochisticmage, catastrophe, it, caused, people, to, get, reckless, and, the, bottom, line, is, that, at, least, three, of, your, friends, will,...   \n",
       "5973                                                                                                                   [nochilllukehammings, im, screaming]   \n",
       "7589                                                                                                                                      [omg, earthquake]   \n",
       "3788  [it's, never, good, sign, when, you, pull, up, to, work, and, there's, five, ambulances, and, fire, truck, in, the, bay, wompppp, at, least, it's,...   \n",
       "825                                                                                               [my, mic, and, controllers, aren't, working, one, second]   \n",
       "\n",
       "                                                                  text_tokenized_no_sw  \n",
       "1489           [masochisticmage, catastrophe, caused, people, reckless, line, friends]  \n",
       "5973                                              [nochilllukehammings, im, screaming]  \n",
       "7589                                                                 [omg, earthquake]  \n",
       "3788  [good, sign, pull, work, there's, ambulances, fire, truck, bay, wompppp, friday]  \n",
       "825                                                [mic, controllers, working, second]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linguistic Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SpaCy to lemmatize our text\n",
    "# Using SpaCy to count NER tags and POS tags\n",
    "# NER tags I'm interested in:\n",
    "## GPE, LOC, NORP, FAC, EVENT, ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-54e8c2b91794>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['lemmas'] = X_train['text_tokenized'].apply(spacy_lemmatize)\n",
      "<ipython-input-42-54e8c2b91794>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['lemmas_no_sw'] = X_train['lemmas'].apply(\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize\n",
    "\n",
    "def spacy_lemmatize(tokens):\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    return [t.lemma_ for t in doc]\n",
    "\n",
    "X_train['lemmas'] = X_train['text_tokenized'].apply(spacy_lemmatize)\n",
    "X_train['lemmas_no_sw'] = X_train['lemmas'].apply(\n",
    "    lambda x: [l for l in x if l not in stopword_list]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_tokenized_no_sw</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>lemmas_no_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>@masochisticmage + catastrophe! it caused people to get reckless and the bottom line is that at least three of your friends will have +</td>\n",
       "      <td>[masochisticmage, catastrophe, it, caused, people, to, get, reckless, and, the, bottom, line, is, that, at, least, three, of, your, friends, will,...</td>\n",
       "      <td>[masochisticmage, catastrophe, caused, people, reckless, line, friends]</td>\n",
       "      <td>[masochisticmage, catastrophe, it, cause, people, to, get, reckless, and, the, bottom, line, be, that, at, least, three, of, your, friend, will, h...</td>\n",
       "      <td>[masochisticmage, catastrophe, cause, people, reckless, line, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>#nochilllukehammings\\nim screaming</td>\n",
       "      <td>[nochilllukehammings, im, screaming]</td>\n",
       "      <td>[nochilllukehammings, im, screaming]</td>\n",
       "      <td>[nochilllukehamming, I, m, scream]</td>\n",
       "      <td>[nochilllukehamming, I, scream]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7589</th>\n",
       "      <td>omg earthquake</td>\n",
       "      <td>[omg, earthquake]</td>\n",
       "      <td>[omg, earthquake]</td>\n",
       "      <td>[omg, earthquake]</td>\n",
       "      <td>[omg, earthquake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>it's never a good sign when you pull up to work and there's five ambulances and a fire truck in the bay. wompppp at least it's friday</td>\n",
       "      <td>[it's, never, good, sign, when, you, pull, up, to, work, and, there's, five, ambulances, and, fire, truck, in, the, bay, wompppp, at, least, it's,...</td>\n",
       "      <td>[good, sign, pull, work, there's, ambulances, fire, truck, bay, wompppp, friday]</td>\n",
       "      <td>[it, be, never, good, sign, when, you, pull, up, to, work, and, there, be, five, ambulance, and, fire, truck, in, the, bay, wompppp, at, least, it...</td>\n",
       "      <td>[good, sign, pull, work, ambulance, fire, truck, bay, wompppp, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>my mic and controllers aren't working one second</td>\n",
       "      <td>[my, mic, and, controllers, aren't, working, one, second]</td>\n",
       "      <td>[mic, controllers, working, second]</td>\n",
       "      <td>[my, mic, and, controller, be, not, work, one, second]</td>\n",
       "      <td>[mic, controller, work, second]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         text  \\\n",
       "1489  @masochisticmage + catastrophe! it caused people to get reckless and the bottom line is that at least three of your friends will have +   \n",
       "5973                                                                                                       #nochilllukehammings\\nim screaming   \n",
       "7589                                                                                                                           omg earthquake   \n",
       "3788    it's never a good sign when you pull up to work and there's five ambulances and a fire truck in the bay. wompppp at least it's friday   \n",
       "825                                                                                          my mic and controllers aren't working one second   \n",
       "\n",
       "                                                                                                                                             text_tokenized  \\\n",
       "1489  [masochisticmage, catastrophe, it, caused, people, to, get, reckless, and, the, bottom, line, is, that, at, least, three, of, your, friends, will,...   \n",
       "5973                                                                                                                   [nochilllukehammings, im, screaming]   \n",
       "7589                                                                                                                                      [omg, earthquake]   \n",
       "3788  [it's, never, good, sign, when, you, pull, up, to, work, and, there's, five, ambulances, and, fire, truck, in, the, bay, wompppp, at, least, it's,...   \n",
       "825                                                                                               [my, mic, and, controllers, aren't, working, one, second]   \n",
       "\n",
       "                                                                  text_tokenized_no_sw  \\\n",
       "1489           [masochisticmage, catastrophe, caused, people, reckless, line, friends]   \n",
       "5973                                              [nochilllukehammings, im, screaming]   \n",
       "7589                                                                 [omg, earthquake]   \n",
       "3788  [good, sign, pull, work, there's, ambulances, fire, truck, bay, wompppp, friday]   \n",
       "825                                                [mic, controllers, working, second]   \n",
       "\n",
       "                                                                                                                                                     lemmas  \\\n",
       "1489  [masochisticmage, catastrophe, it, cause, people, to, get, reckless, and, the, bottom, line, be, that, at, least, three, of, your, friend, will, h...   \n",
       "5973                                                                                                                     [nochilllukehamming, I, m, scream]   \n",
       "7589                                                                                                                                      [omg, earthquake]   \n",
       "3788  [it, be, never, good, sign, when, you, pull, up, to, work, and, there, be, five, ambulance, and, fire, truck, in, the, bay, wompppp, at, least, it...   \n",
       "825                                                                                                  [my, mic, and, controller, be, not, work, one, second]   \n",
       "\n",
       "                                                                lemmas_no_sw  \n",
       "1489   [masochisticmage, catastrophe, cause, people, reckless, line, friend]  \n",
       "5973                                         [nochilllukehamming, I, scream]  \n",
       "7589                                                       [omg, earthquake]  \n",
       "3788  [good, sign, pull, work, ambulance, fire, truck, bay, wompppp, friday]  \n",
       "825                                          [mic, controller, work, second]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy POS\n",
    "\n",
    "def helper_untokenize(token_list):\n",
    "    \"\"\"\n",
    "    Helper function.\n",
    "    Takes in a list of tokens and combines them as a string.\n",
    "    Instead of having to repeatedly type `lambda x: ' '.join()`\n",
    "    \"\"\"\n",
    "    return ' '.join(token_list)\n",
    "\n",
    "def helper_spacy_pos(text):\n",
    "    \"\"\"\n",
    "    Helper function.\n",
    "    Takes in a string and returns a list of part-of-speech tokens.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    pos_tokens = [t.pos_ for t in doc]\n",
    "    return pos_tokens\n",
    "\n",
    "def make_pos_tokens_df(text_column):\n",
    "    \"\"\"\n",
    "    Takes in a Series of texts.\n",
    "    Uses SpaCy to turn the text into part-of-speech tokens.\n",
    "    Uses sklearn's CountVectorizer to count POS tags for each text. \n",
    "    \"\"\"\n",
    "    cvec = CountVectorizer(tokenizer=helper_spacy_pos)\n",
    "    pos_vectorized = cvec.fit_transform(text_column)\n",
    "    pos_vectorized_df = pd.DataFrame(\n",
    "        pos_vectorized.toarray(),\n",
    "        columns=cvec.get_feature_names(),\n",
    "        index=text_column.index\n",
    "        )\n",
    "    return pos_vectorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>AUX</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PART</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7589</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ADJ  ADP  ADV  AUX  CCONJ  DET  INTJ  NOUN  NUM  PART  PRON  PROPN  \\\n",
       "1489    2    1    2    2      1    1     0     5    1     1     2      0   \n",
       "5973    0    0    0    1      0    0     0     1    0     0     1      0   \n",
       "7589    0    0    0    0      0    0     0     1    0     0     0      1   \n",
       "3788    2    4    1    2      2    1     0     6    1     0     4      2   \n",
       "825     0    0    0    1      1    0     0     3    1     1     1      0   \n",
       "\n",
       "      PUNCT  SCONJ  VERB  X  \n",
       "1489      0      1     3  0  \n",
       "5973      0      0     1  0  \n",
       "7589      0      0     0  0  \n",
       "3788      0      1     2  0  \n",
       "825       0      0     1  0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vectorized_df_train = make_pos_tokens_df(X_train['text_tokenized'].apply(helper_untokenize))\n",
    "pos_vectorized_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy NER\n",
    "\n",
    "def helper_spacy_ner(\n",
    "    text, \n",
    "    ner_tags=['GPE', 'LOC', 'NORP', 'EVENT', 'ORG', 'FAC']\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Helper function.\n",
    "    Takes in a string and returns a list of named-entity recognition tags.\n",
    "    Also takes in a specific list of NER tags to look for.\n",
    "    To look for all NER tags supported by SpaCy, set `ner_tags=None`.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    ents = doc.ents\n",
    "    if ner_tags:\n",
    "        tags = [ent.label_ for ent in doc.ents if ent.label_ in ner_tags]\n",
    "        return tags\n",
    "    else:\n",
    "        tags = [ent.label_ for ent in doc.ents]\n",
    "        return tags\n",
    "\n",
    "def make_ner_tokens_df(text_column):\n",
    "    \"\"\"\n",
    "    Takes in a Series of texts.\n",
    "    Uses SpaCy to turn the text a list of named entity recognition tags.\n",
    "    Uses sklearn's CountVectorizer to count NER tags for each text.\n",
    "    \"\"\"\n",
    "    cvec = CountVectorizer(tokenizer=helper_spacy_ner)\n",
    "    ner_vectorized = cvec.fit_transform(text_column)\n",
    "    ner_vectorized_df = pd.DataFrame(\n",
    "        ner_vectorized.toarray(),\n",
    "        columns=cvec.get_feature_names(),\n",
    "        index=text_column.index\n",
    "    )\n",
    "    return ner_vectorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENT</th>\n",
       "      <th>FAC</th>\n",
       "      <th>GPE</th>\n",
       "      <th>LOC</th>\n",
       "      <th>NORP</th>\n",
       "      <th>ORG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7589</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      EVENT  FAC  GPE  LOC  NORP  ORG\n",
       "1489      0    0    0    0     0    0\n",
       "5973      0    0    0    0     0    0\n",
       "7589      0    0    0    0     0    0\n",
       "3788      0    0    0    1     0    0\n",
       "825       0    0    0    0     0    0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_vectorized_df_train = make_ner_tokens_df(X_train['text_tokenized'].apply(helper_untokenize))\n",
    "ner_vectorized_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Feature engineering\n",
    "\n",
    "# include or exclude stop words?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has_url_Series already exists\n",
    "# Start a DF with it\n",
    "\n",
    "engineered_features_df_train = pd.DataFrame(has_url_Series_train)\n",
    "\n",
    "# Character count. Original tweet. No URL.\n",
    "engineered_features_df_train['character_count'] = \\\n",
    "    X_train['text'].apply(len)\n",
    "\n",
    "# Token count. Including stop words.\n",
    "engineered_features_df_train['token_count'] = \\\n",
    "    X_train['text_tokenized'].apply(len)\n",
    "\n",
    "# Number of unique tokens. Including stop words.\n",
    "engineered_features_df_train['unique_tokens'] = \\\n",
    "    X_train['text_tokenized'].apply(lambda x: len(set(x)))\n",
    "\n",
    "# Number of lemmas\n",
    "# Unique lemmas\n",
    "# Average lemma length\n",
    "\n",
    "# Average token length. Including stop words.\n",
    "engineered_features_df_train['mean_token_length'] = \\\n",
    "    X_train['text_tokenized'].apply(lambda x: np.mean([len(t) for t in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e508bc85c11e03fe10d0c37e7996d01ac3bac0c8b57b70908c892ac1dabf0c16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
