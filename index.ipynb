{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Diesaster Tweets ******\n",
    "Flatiron School Data Science: Project 4\n",
    "\n",
    "Advanced Machine Learning Topics **********\n",
    "- **Author**: Zaid Shoorbajee\n",
    "- **Instructor**: Morgan Jones\n",
    "- **Pace**: Flex, 40 weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Business Understanding\n",
    "Lorem ipsum\n",
    "## Data Undersanding\n",
    "Lorem ipsum\n",
    "### Dataset and features\n",
    "Lorem ipsum\n",
    "### Target variable\n",
    "Lorem ipsum\n",
    "### Scoring\n",
    "Lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import \\\n",
    "    regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict, defaultdict, Counter\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_colwidth = 150\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to revise headers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in the woods...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now in the building across the street</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our area...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "5   8     NaN      NaN   \n",
       "6  10     NaN      NaN   \n",
       "7  13     NaN      NaN   \n",
       "8  14     NaN      NaN   \n",
       "9  15     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "5                         #RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires   \n",
       "6                                        #flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas   \n",
       "7                                                                            I'm on top of the hill and I can see a fire in the woods...   \n",
       "8                                                        There's an emergency evacuation happening now in the building across the street   \n",
       "9                                                                                   I'm afraid that the tornado is coming to our area...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading and previewing the dataset\n",
    "\n",
    "df = pd.read_csv('./data/disaster_tweets/train.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword and location columns\n",
    "\n",
    "The `location` column doesn't have much usable information in some cases it's just nonsense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of some of the location values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['World Wide!!',\n",
       " 'Paranaque City',\n",
       " 'Live On Webcam',\n",
       " 'milky way',\n",
       " 'GREENSBORO,NORTH CAROLINA',\n",
       " 'England.',\n",
       " 'Sheffield Township, Ohio',\n",
       " 'India',\n",
       " 'Barbados',\n",
       " 'Anaheim']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sample of some of the location values')\n",
    "df['location'].unique()[7:17].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore I can't use the `location` column without some pre-processing. I will fill the missing values with `location_missing` for now. Later, when I'm processing more of the text data, I'll extract some information from this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location'] = df['location'].fillna('location_missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `keyword` column shows what was used to search for relevant tweets. This column can give use insight as to what kinds of tweets the keywords yield. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                      61\n",
       "fatalities               45\n",
       "deluge                   42\n",
       "armageddon               42\n",
       "harm                     41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 222, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keyword'].value_counts(dropna=False)\n",
    "# Counter(df['keyword'])[np.NaN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keyword` has 61 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with 'missing_keyword' so I can visualize.\n",
    "\n",
    "df['keyword'] = df['keyword'].fillna('missing_keyword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keyword'] = df['keyword'].apply(lambda x: x.replace('%20', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['keyword_count_0'] = \\\n",
    "#     df['keyword'].apply(\n",
    "#         lambda x: Counter(df['keyword'][df['target'] == 0])[x]\n",
    "#         )\n",
    "# df['keyword_count_1'] = \\\n",
    "#     df['keyword'].apply(\n",
    "#         lambda x: Counter(df['keyword'][df['target'] == 1])[x]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=2, figsize=(15, 50))\n",
    "\n",
    "# sns.countplot(\n",
    "#     y=df[df['target']==0].sort_values(\n",
    "#         by='keyword_count_0', ascending=False\n",
    "#         )['keyword'],\n",
    "#     color='blue',\n",
    "#     ax=axes[0]\n",
    "#     )\n",
    "# sns.countplot(\n",
    "#     y=df[df['target']==1].sort_values(\n",
    "#         by='keyword_count_1', ascending=False\n",
    "#         )['keyword'],\n",
    "#     color='red',\n",
    "#     ax=axes[1]\n",
    "#     )\n",
    "\n",
    "# axes[0].set_title('Non-disaster tweets')\n",
    "# axes[1].set_title('Disaster tweets')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets and Target\n",
    "The main feature is `text`, which is the full text of the given tweet. Each tweet is labeled in `target` as referring to a disaster (1) or not (0).\n",
    "#### Duplicate tweets\n",
    "Before proceeding, I'll check if there are any tweets that are duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous duplicate tweets, perhaps tweeted by different accounts. The most concerning part of this is that some of these duplicates have contradicting labels. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>4656</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>location_missing</td>\n",
       "      <td>He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>4659</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>4669</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>Bahrain</td>\n",
       "      <td>He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>4672</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>location_missing</td>\n",
       "      <td>He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>4684</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>location_missing</td>\n",
       "      <td>He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266</th>\n",
       "      <td>4691</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>location_missing</td>\n",
       "      <td>He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   keyword          location  \\\n",
       "3240  4656  engulfed  location_missing   \n",
       "3243  4659  engulfed           Kuwait    \n",
       "3248  4669  engulfed           Bahrain   \n",
       "3251  4672  engulfed  location_missing   \n",
       "3261  4684  engulfed  location_missing   \n",
       "3266  4691  engulfed  location_missing   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "3240  He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam   \n",
       "3243  He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam   \n",
       "3248  He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam   \n",
       "3251  He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam   \n",
       "3261  He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam   \n",
       "3266  He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam   \n",
       "\n",
       "      target  \n",
       "3240       0  \n",
       "3243       1  \n",
       "3248       1  \n",
       "3251       0  \n",
       "3261       0  \n",
       "3266       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>4068</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>4072</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>4076</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>4077</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    keyword                  location  \\\n",
       "2830  4068  displaced  Pedophile hunting ground   \n",
       "2831  4072  displaced  Pedophile hunting ground   \n",
       "2832  4076  displaced  Pedophile hunting ground   \n",
       "2833  4077  displaced  Pedophile hunting ground   \n",
       "\n",
       "                                                                                                                                        text  \\\n",
       "2830  .POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4   \n",
       "2831  .POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4   \n",
       "2832  .POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4   \n",
       "2833  .POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4   \n",
       "\n",
       "      target  \n",
       "2830       1  \n",
       "2831       1  \n",
       "2832       0  \n",
       "2833       1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text'] == \".POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4232</th>\n",
       "      <td>6012</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>location_missing</td>\n",
       "      <td>Caution: breathing may be hazardous to your health.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>6017</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>location_missing</td>\n",
       "      <td>Caution: breathing may be hazardous to your health.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    keyword          location  \\\n",
       "4232  6012  hazardous  location_missing   \n",
       "4235  6017  hazardous  location_missing   \n",
       "\n",
       "                                                     text  target  \n",
       "4232  Caution: breathing may be hazardous to your health.       1  \n",
       "4235  Caution: breathing may be hazardous to your health.       0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text'] == \"Caution: breathing may be hazardous to your health.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>1760</td>\n",
       "      <td>buildings burning</td>\n",
       "      <td>dallas</td>\n",
       "      <td>like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>1950</td>\n",
       "      <td>burning buildings</td>\n",
       "      <td>dallas</td>\n",
       "      <td>like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id            keyword location  \\\n",
       "1221  1760  buildings burning   dallas   \n",
       "1349  1950  burning buildings   dallas   \n",
       "\n",
       "                                                                                                                               text  \\\n",
       "1221  like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit   \n",
       "1349  like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit   \n",
       "\n",
       "      target  \n",
       "1221       1  \n",
       "1349       0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text'] == \"like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that some of these tweets were labeled sloppily or are difficult to interpret. In any case, having identical tweets labeled differently will cause unwanted noise in the model. Therefore, I'll drop all duplicated tweets, which account for less than 2% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0144489688690398"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proportion of duplicate tweets\n",
    "\n",
    "df['text'].duplicated().sum() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset = 'text', keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating features from target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df.drop(columns='target')\n",
    "target = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Target\n",
      "\n",
      "0    4284\n",
      "1    3150\n",
      "Name: target, dtype: int64\n",
      "\n",
      "Normalized:\n",
      "0    0.576271\n",
      "1    0.423729\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Distribution of Target')\n",
    "print()\n",
    "print(target.value_counts())\n",
    "print()\n",
    "print('Normalized:')\n",
    "print(target.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 42% of the tweets have been labeled as actual disaster tweets. This is not a major imbalance, and this I won't need to artificially rebalance the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the full dataset into training and testing data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tweets, target, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "# Splitting off a validation set\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=.5, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39                 ablaze\n",
       "418              arsonist\n",
       "4916               mayhem\n",
       "2126               deaths\n",
       "1054            body bags\n",
       "              ...        \n",
       "5848                 ruin\n",
       "2600            destroyed\n",
       "545             avalanche\n",
       "1245    buildings on fire\n",
       "4399            hijacking\n",
       "Name: keyword, Length: 5947, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore', sparse=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_keyword = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "ohe_keyword.fit(X_train[['keyword']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_ablaze</th>\n",
       "      <th>x0_accident</th>\n",
       "      <th>x0_aftershock</th>\n",
       "      <th>x0_airplane accident</th>\n",
       "      <th>x0_ambulance</th>\n",
       "      <th>x0_annihilated</th>\n",
       "      <th>x0_annihilation</th>\n",
       "      <th>x0_apocalypse</th>\n",
       "      <th>x0_armageddon</th>\n",
       "      <th>x0_army</th>\n",
       "      <th>...</th>\n",
       "      <th>x0_weapons</th>\n",
       "      <th>x0_whirlwind</th>\n",
       "      <th>x0_wild fires</th>\n",
       "      <th>x0_wildfire</th>\n",
       "      <th>x0_windstorm</th>\n",
       "      <th>x0_wounded</th>\n",
       "      <th>x0_wounds</th>\n",
       "      <th>x0_wreck</th>\n",
       "      <th>x0_wreckage</th>\n",
       "      <th>x0_wrecked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5848</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5947 rows Ã— 222 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      x0_ablaze  x0_accident  x0_aftershock  x0_airplane accident  \\\n",
       "39          1.0          0.0            0.0                   0.0   \n",
       "418         0.0          0.0            0.0                   0.0   \n",
       "4916        0.0          0.0            0.0                   0.0   \n",
       "2126        0.0          0.0            0.0                   0.0   \n",
       "1054        0.0          0.0            0.0                   0.0   \n",
       "...         ...          ...            ...                   ...   \n",
       "5848        0.0          0.0            0.0                   0.0   \n",
       "2600        0.0          0.0            0.0                   0.0   \n",
       "545         0.0          0.0            0.0                   0.0   \n",
       "1245        0.0          0.0            0.0                   0.0   \n",
       "4399        0.0          0.0            0.0                   0.0   \n",
       "\n",
       "      x0_ambulance  x0_annihilated  x0_annihilation  x0_apocalypse  \\\n",
       "39             0.0             0.0              0.0            0.0   \n",
       "418            0.0             0.0              0.0            0.0   \n",
       "4916           0.0             0.0              0.0            0.0   \n",
       "2126           0.0             0.0              0.0            0.0   \n",
       "1054           0.0             0.0              0.0            0.0   \n",
       "...            ...             ...              ...            ...   \n",
       "5848           0.0             0.0              0.0            0.0   \n",
       "2600           0.0             0.0              0.0            0.0   \n",
       "545            0.0             0.0              0.0            0.0   \n",
       "1245           0.0             0.0              0.0            0.0   \n",
       "4399           0.0             0.0              0.0            0.0   \n",
       "\n",
       "      x0_armageddon  x0_army  ...  x0_weapons  x0_whirlwind  x0_wild fires  \\\n",
       "39              0.0      0.0  ...         0.0           0.0            0.0   \n",
       "418             0.0      0.0  ...         0.0           0.0            0.0   \n",
       "4916            0.0      0.0  ...         0.0           0.0            0.0   \n",
       "2126            0.0      0.0  ...         0.0           0.0            0.0   \n",
       "1054            0.0      0.0  ...         0.0           0.0            0.0   \n",
       "...             ...      ...  ...         ...           ...            ...   \n",
       "5848            0.0      0.0  ...         0.0           0.0            0.0   \n",
       "2600            0.0      0.0  ...         0.0           0.0            0.0   \n",
       "545             0.0      0.0  ...         0.0           0.0            0.0   \n",
       "1245            0.0      0.0  ...         0.0           0.0            0.0   \n",
       "4399            0.0      0.0  ...         0.0           0.0            0.0   \n",
       "\n",
       "      x0_wildfire  x0_windstorm  x0_wounded  x0_wounds  x0_wreck  x0_wreckage  \\\n",
       "39            0.0           0.0         0.0        0.0       0.0          0.0   \n",
       "418           0.0           0.0         0.0        0.0       0.0          0.0   \n",
       "4916          0.0           0.0         0.0        0.0       0.0          0.0   \n",
       "2126          0.0           0.0         0.0        0.0       0.0          0.0   \n",
       "1054          0.0           0.0         0.0        0.0       0.0          0.0   \n",
       "...           ...           ...         ...        ...       ...          ...   \n",
       "5848          0.0           0.0         0.0        0.0       0.0          0.0   \n",
       "2600          0.0           0.0         0.0        0.0       0.0          0.0   \n",
       "545           0.0           0.0         0.0        0.0       0.0          0.0   \n",
       "1245          0.0           0.0         0.0        0.0       0.0          0.0   \n",
       "4399          0.0           0.0         0.0        0.0       0.0          0.0   \n",
       "\n",
       "      x0_wrecked  \n",
       "39           0.0  \n",
       "418          0.0  \n",
       "4916         0.0  \n",
       "2126         0.0  \n",
       "1054         0.0  \n",
       "...          ...  \n",
       "5848         0.0  \n",
       "2600         0.0  \n",
       "545          0.0  \n",
       "1245         0.0  \n",
       "4399         0.0  \n",
       "\n",
       "[5947 rows x 222 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ohe_keyword.transform(X_train[['keyword']]), index=X_train.index, columns=ohe_keyword.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End Secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and tokenizing the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a copy of untouched tweets\n",
    "X_train_tweets_unprocessed = X_train.copy()['text'] \n",
    "\n",
    "X_test_tweets_unprocessed = X_test.copy()['text'] \n",
    "\n",
    "X_val_tweets_unprocessed = X_val.copy()['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make everything lowercase\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with URLs\n",
    "\n",
    "Many tweets contain URLs, which, from an NLP standpoint are essentially random strings and thus won't be useful as vectorized tokens. But before I remove them, I will turn the presence of a URL into a binary feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_url(text):\n",
    "    \"\"\"\n",
    "    Returns 1 if a string contains a URL, else returns 0.\n",
    "    \"\"\"\n",
    "    search = re.search(pattern=r'http\\S+', string=text)\n",
    "    return int(bool(search))\n",
    "\n",
    "# Making a series indicating whether the tweet has a URL. I will use this later when extracting more meta-features.\n",
    "\n",
    "has_url_Series_train = X_train['text'].apply(binary_url).rename('has_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing URLs from all tweets\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(\n",
    "    lambda x: re.sub(\n",
    "        pattern=r'http\\S+', repl='', string=x)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing tweets\n",
    "\n",
    "#### I plan to make multiple tokenized versions of each tweet.\n",
    "* Basic version: Any word with at least two letters. Strips the symbols for hashtags (#) and mentions(@)\n",
    "* Basic version, excluding stop words\n",
    "* Lemmatized version of basic version\n",
    "* Lemmatized version of basic version, exluding stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweet = X_train['text'].loc[1245]\n",
    "example_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = r\"[a-zA-Z]+'?[a-zA-Z]+|\\b[iIaA]\\b\"\n",
    "# Pattern: Any word with at least two characters, including up to one apostrophe\n",
    "# Also captures the English words \"I\" and \"and\".\n",
    "\n",
    "tokenizer = RegexpTokenizer(token_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the tokenizer does with the pattern I used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(example_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to work. However, 'amp' is an artifact of \"&amp;\", which is the HTML entity for an ampersand (\"&\").\n",
    "\n",
    "I will replace any instance of \"&amp;\" with the word \"and\" in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'] = X_train['text'].apply(lambda x: re.sub(\"&amp;\", \"and\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['tokens'] = X_train['text'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check most common tokens\n",
    "\n",
    "top_20_tokens = FreqDist(X_train['tokens'].explode()).most_common(20)\n",
    "top_20_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All tweets have been tokenized. Now I will make a version without stop words.\n",
    "\n",
    "To make a comprehensive list of stop words, I will combine the default lists from the NLTK and SpaCy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_spacy = list(nlp.Defaults.stop_words)\n",
    "sw_nltk = stopwords.words('english')\n",
    "stopword_list = list(set(sw_spacy + sw_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['tokens_no_sw'] = \\\n",
    "    X_train['tokens'].apply(\n",
    "        lambda x: [w for w in x if not w in stopword_list]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check most common tokens (without stop words)\n",
    "\n",
    "FreqDist(X_train['tokens_no_sw'].explode()).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"i'm\" should also be considered a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list.extend([\"i'm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['tokens_no_sw'] = \\\n",
    "    X_train['tokens'].apply(\n",
    "        lambda x: [w for w in x if not w in stopword_list]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check most common tokens (without stop words)\n",
    "\n",
    "top_20_tokens_no_sw = FreqDist(X_train['tokens_no_sw'].explode()).most_common(20)\n",
    "top_20_tokens_no_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previewing the restulting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing tweets\n",
    "\n",
    "Now I want to make a version of these tokenized tweets where each word is lemmatized. **Lemmatization** is _________.\n",
    "\n",
    "I will the tokenizer I made above, then the SpaCy library's lemmatizer to do this. Essentially I am making my own tokenizing function, where the output is lemmatized tokens instead of just plain tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Uses NLTK and SpaCy to tokenize a string and return the lemma of each token.\n",
    "    \"\"\"\n",
    "    sents = [s.text for s in nlp(text).sents]\n",
    "    sents_tokenized = [tokenizer.tokenize(sent) for sent in sents]\n",
    "    docs = [nlp(' '.join(tokens)) for tokens in sents_tokenized]\n",
    "    docs_lemmatized = [[t.lemma_.lower() for t in doc] for doc in docs]\n",
    "    return list(itertools.chain.from_iterable(docs_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized tokens, including stop words\n",
    "\n",
    "X_train['lemmas'] = X_train['text'].apply(spacy_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check most common lemmas\n",
    "\n",
    "top_20_lemmas = FreqDist(X_train['lemmas'].explode()).most_common(20)\n",
    "top_20_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the lemmatized tokens without stop words, I will also need to lemmatize the stop words.\n",
    "\n",
    "This has to be done to the stop words while they are still within the string because  SpaCy uses grammatical context to lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized version of stop words based on training set\n",
    "\n",
    "stopword_list_lem = []\n",
    "\n",
    "sw_lem_Series = \\\n",
    "    X_train['tokens'].apply(\n",
    "    lambda x: [t.lemma_.lower() for t in nlp(' '.join(x)) if t.text in stopword_list]\n",
    "    )\n",
    "\n",
    "for row in sw_lem_Series:\n",
    "    stopword_list_lem.extend(row)\n",
    "\n",
    "stopword_list_lem = list(set(stopword_list_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized tokens, excluding stop words\n",
    "\n",
    "X_train['lemmas_no_sw'] = X_train['lemmas'].apply(\n",
    "    lambda x: [l for l in x if l not in stopword_list_lem]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check most common lemmas (without stop words)\n",
    "\n",
    "top_20_lemmas_no_sw = FreqDist(X_train['lemmas_no_sw'].explode()).most_common(20)\n",
    "top_20_lemmas_no_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing and comparing frequency\n",
    "Here, I'll visualize the top 20 words in `X_train` for disaster tweets vs. non-disaster tweets. I should be able to see which words the classes have in common, as well as which processed version of the tweets is most different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_freqdict_classes(series, y, cutoff=20):\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "    fd_0 = FreqDist(series[y==0].explode()).most_common(cutoff)\n",
    "    fd_0 = OrderedDict(fd_0)\n",
    "    tokens_0 = list(fd_0.keys())[::-1]\n",
    "    freq_0 = list(fd_0.values())[::-1]\n",
    "    fd_1 = FreqDist(series[y==1].explode()).most_common(cutoff)\n",
    "    fd_1 = OrderedDict(fd_1)\n",
    "    tokens_1 = list(fd_1.keys())[::-1]\n",
    "    freq_1 = list(fd_1.values())[::-1]\n",
    "    shared_tokens = [t for t in tokens_0 if t in tokens_1]\n",
    "    axes[0].barh(y=tokens_0, width=freq_0, color=['C6' if token in shared_tokens else 'C0' for token in tokens_0])\n",
    "    axes[1].barh(y=tokens_1, width=freq_1, color=['C6' if token in shared_tokens else 'C0' for token in tokens_1])\n",
    "    axes[0].set_ylabel('Tokens', size=10)\n",
    "    axes[0].set_xlabel('Frequency', size=10)\n",
    "    axes[1].set_xlabel('Frequency', size=10)\n",
    "    axes[0].set_title(f'Top {cutoff} {series.name.upper()} (Non-Disaster)')\n",
    "    axes[1].set_title(f'Top {cutoff} {series.name.upper()} (Disaster)')\n",
    "    custom_bars = [Line2D([0], [0], color='C6', lw=10), Line2D([0], [0], color='C0', lw=10)]\n",
    "    axes[0].legend(custom_bars, ['In common', 'Not in common'])\n",
    "    axes[1].legend(custom_bars, ['In common', 'Not in common'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freqdict_classes(X_train['tokens'], y_train)\n",
    "plot_freqdict_classes(X_train['tokens_no_sw'], y_train)\n",
    "plot_freqdict_classes(X_train['lemmas'], y_train)\n",
    "plot_freqdict_classes(X_train['lemmas_no_sw'], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can tell from the charts above the version of tweets that are the most different in terms of token frequency is the **lemmatized tweets with no stop words.** In the top 20 tokens from each class, there are only two shared tokens. In contrast, if stop words are not removed, most tokens are shared between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More linguistic feature engineering\n",
    "\n",
    "I will use the SpaCy library to extract more linguistic features from the tweets.\n",
    "\n",
    "### Vectorized part of speech (POS) tags\n",
    "\n",
    "The SpaCy library is pre-trained to parse through sentences and identify each word's grammatical part of speech.\n",
    "\n",
    "Here are some examples of what the tool can identify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS VISUALIZATIONS\n",
    "#\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_sample = X_train['text'].sample(5).apply(lambda x: [s.text for s in nlp(x).sents]).apply(lambda x: [' '.join(tokenizer.tokenize(s)) for s in x]).apply(lambda x: '. '.join(x))\n",
    "secret_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_displacy = {'compact':True, 'distance':110, 'bg':'#3056ff', 'color':'fff'}\n",
    "\n",
    "for tweet in secret_sample:\n",
    "    displacy.render(nlp(tweet), style='dep', jupyter=True, options=options_displacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plan to use this tool in order to vectorize the detailed [parts of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) of each tweet.\n",
    "\n",
    "First, I'll convert each token into a string of its POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy POS\n",
    "\n",
    "def spacy_pos(text):\n",
    "    \"\"\"\n",
    "    Takes in a string and returns a list of part of speech tokens.\n",
    "    \"\"\"\n",
    "    sents = [s.text for s in nlp(text).sents]\n",
    "    sents_tokenized = [tokenizer.tokenize(sent) for sent in sents]\n",
    "    docs = [nlp(' '.join(tokens)) for tokens in sents_tokenized]\n",
    "    pos_tokens = [[t.pos_ for t in doc] for doc in docs]\n",
    "    return list(itertools.chain.from_iterable(pos_tokens))\n",
    "\n",
    "X_train['text_as_POS'] = X_train['text'].apply(spacy_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the POS vectors, I'm using scikit-learn's CountVectorizer in a slightly unorthodox way. I'm using it to count POS tags rather than tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataframes of vectorized POS tags\n",
    "\n",
    "pos_vectorizer = CountVectorizer(tokenizer=spacy_pos)\n",
    "pos_vec_train = pos_vectorizer.fit_transform(X_train['text'])\n",
    "pos_vec_df_train = pd.DataFrame(\n",
    "        pos_vec_train.toarray(),\n",
    "        columns=pos_vectorizer.get_feature_names(),\n",
    "        index=X_train.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vec_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix of parts-of speech will hopefully be useful to the model, but I can take it even further. I'm going to look at disparities in the proportions of POS tags between each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vec_df_norm_train = pos_vec_df_train.div(pos_vec_df_train.sum(axis=1), axis=0)\n",
    "pos_vec_df_norm_train.columns = pos_vec_df_train.columns + '_norm'\n",
    "pos_vec_df_norm_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,7))\n",
    "\n",
    "pos_plot_0 = pos_vec_df_norm_train[y_train==0].mean().plot(\n",
    "    kind='barh',\n",
    "    ax=ax,\n",
    "    color='blue', alpha=0.5,\n",
    "    label='Non-disaster'\n",
    ")\n",
    "pos_plot_1 = pos_vec_df_norm_train[y_train==1].mean().plot(\n",
    "    kind='barh',\n",
    "    ax=ax,\n",
    "    color='orange', alpha=0.5,\n",
    "    label='Disaster'\n",
    ")\n",
    "ax.set_title('Average proportion of POS tags in each tweet')\n",
    "ax.set_yticklabels(pos_vec_df_train.columns)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the two classes, there are noticeable disparities with the following POS tags:\n",
    "\n",
    "* ADP: adposition\n",
    "* ADV: adverb\n",
    "* AUX: auxiliary\n",
    "* NOUN: Noun\n",
    "* PRON: Pronoun\n",
    "* PROPN: Proper noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_tags = ['ADP_norm', 'ADV_norm', 'AUX_norm', 'NOUN_norm', 'PRON_norm', 'PROPN_norm']\n",
    "\n",
    "pos_vec_df_train = pos_vec_df_train.join(pos_vec_df_norm_train[interesting_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vec_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE THIS A SEPARATE DF OR TACK ONTO META?\n",
    "# CHANGE ORDER OF META AND LINGUISTIC?\n",
    "#\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE?\n",
    "# PROPORTIONS?\n",
    "#\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized named-entity recognition (NER) tags \n",
    "SpaCy has the capability of recognizing \"named-entities\" such as places, companies, dates, people, and more. Here are some examples of what the tool can identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE NER EXAMPLES\n",
    "#\n",
    "# \n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of identifying disaster tweets, here are the [NER tags](https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218) I am interested in:\n",
    "\n",
    "* **GPE**: Countries, cities, states.\n",
    "* **LOC**: Non-GPE locations, mountain ranges, bodies of water.\n",
    "* **NORP**: Nationalities or religious or political groups.\n",
    "* **ORG**: Companies, agencies, institutions, etc.\n",
    "\n",
    "I am again using scikit-learn's CountVectorizer to count the entities that SpaCy finds in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy NER\n",
    "\n",
    "def spacy_ner(\n",
    "    text, \n",
    "    ner_tags=['GPE', 'NORP', 'ORG', 'LOC']\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Takes in a string and returns a list of named-entity recognition tags.\n",
    "    Also takes in a specific list of NER tags to look for.\n",
    "    To look for all NER tags supported by SpaCy, set `ner_tags=None`.\n",
    "    Intended use is to use this function as a tokenizer in an sklearn vectorizor.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    ents = doc.ents\n",
    "    if ner_tags:\n",
    "        tags = [ent.label_ for ent in ents if ent.label_ in ner_tags]\n",
    "        return tags\n",
    "    else:\n",
    "        tags = [ent.label_ for ent in ents]\n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataframes of vectorized NER tags\n",
    "\n",
    "ner_vectorizer = CountVectorizer(tokenizer=spacy_ner)\n",
    "ner_vec_train = ner_vectorizer.fit_transform(X_train['text'])\n",
    "ner_vec_df_train = pd.DataFrame(\n",
    "        ner_vec_train.toarray(),\n",
    "        columns=ner_vectorizer.get_feature_names(),\n",
    "        index=X_train.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_vec_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NER to encode the `location` column\n",
    "\n",
    "As noted earlier, the `location` column contains a lot of user-generated nonsense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample of some of the location values')\n",
    "df['location'].unique()[7:17].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But some of its data may be genuine. I can use named-entity recognition to discern if an entry is referring to an actual location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find any NER tags in the location column\n",
    "\n",
    "location_NER_train = X_train['location'].apply(lambda x: [ent.label_ for ent in nlp(x).ents])\n",
    "location_NER_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize based on if the location returns a GPE tag (countries, cities, states)\n",
    "# Add this new feature to the NER training set \n",
    "\n",
    "ner_vec_df_train['location_GPE'] = location_NER_train.apply(lambda x: int('GPE' in x))\n",
    "ner_vec_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-feature Engineering\n",
    "\n",
    "To engineer more features, I'm going to use seemingly arbitrary information from each tweet. Here's the set of meta-features I plan to make:\n",
    "\n",
    "* Has URL (binary)\n",
    "* Character count\n",
    "* Number of stop words\n",
    "* Character count of non-stop-words / total character count\n",
    "* Average length of tokens\n",
    "* Number of tokens\n",
    "* Number of unique tokens\n",
    "* Proportion of stop words\n",
    "* Proportion of words that are hashtags (#)\n",
    "* Proportion of words that are mentions (@)\n",
    "\n",
    "I drew inspiration for some of these features from [this Kaggle entry](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert/notebook).\n",
    "\n",
    "With the meta-features that are token-oriented, I have options. I have four different versions of tokenized tweets:\n",
    "\n",
    "* tokens\n",
    "* tokens without stop words\n",
    "* lemmas\n",
    "* lemmas without stop words\n",
    "\n",
    "In order to pick the version of tokens that will likely be most informative to the model, I will run statistical t-tests on each of them. I'm trying to answer the question: **Which version of tokens, when used to engineer a new feature, has the biggest disparity between classes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average length of token\n",
    "Based on the results of the t-tests below, `lemmas` is the best column to use to engineer this feature. It has the largest t-statistic when the two classes are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for column in ['tokens','tokens_no_sw', 'lemmas', 'lemmas_no_sw']:\n",
    "\n",
    "    tokens = X_train[column][X_train[column].apply(lambda x: x != [])]\n",
    "\n",
    "    average_length_0 = \\\n",
    "        tokens[y_train==0].apply(lambda x: np.mean([len(t) for t in x]))\n",
    "    average_length_1 = \\\n",
    "        tokens[y_train==1].apply(lambda x: np.mean([len(t) for t in x]))\n",
    "\n",
    "    t_test = ttest_ind(average_length_0, average_length_1)\n",
    "    print(f'{column:{20}}{t_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of tokens\n",
    "`lemmas_no_sw` is the best column to use to engineer this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['tokens','tokens_no_sw', 'lemmas', 'lemmas_no_sw']:\n",
    "\n",
    "    n_tokens_0 = \\\n",
    "        X_train[column][y_train==0].apply(len)\n",
    "    n_tokens_1 = \\\n",
    "        X_train[column][y_train==1].apply(len)\n",
    "        \n",
    "    t_test = ttest_ind(n_tokens_0, n_tokens_1)\n",
    "    print(f'{column:{20}}{t_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique tokens\n",
    "`lemmas_no_sw` is the best column to use to engineer this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['tokens','tokens_no_sw', 'lemmas', 'lemmas_no_sw']:\n",
    "\n",
    "    unique_tokens_0 = \\\n",
    "        X_train[column][y_train==0].apply(lambda x: len(set(x)))\n",
    "    unique_tokens_1 = \\\n",
    "        X_train[column][y_train==1].apply(lambda x: len(set(x)))\n",
    "        \n",
    "    t_test = ttest_ind(unique_tokens_0, unique_tokens_1)\n",
    "    print(f'{column:{20}}{t_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will make a new DataFrame composed of the meta-features I listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has_url_Series already exists\n",
    "# Start a DF with it\n",
    "\n",
    "meta_features_df_train = pd.DataFrame(has_url_Series_train)\n",
    "\n",
    "# Character count. Original tweet. No URL.\n",
    "meta_features_df_train['character_count'] = \\\n",
    "    X_train['text'].apply(len)\n",
    "\n",
    "# Stop word count. Original tweet. No URL.\n",
    "meta_features_df_train['sw_count'] = \\\n",
    "    X_train['tokens'].apply(lambda x: len([w for w in x if w in stopword_list]))\n",
    "\n",
    "# Character count of non-stop-words / original character count. No URL.\n",
    "meta_features_df_train['non_sw_char_proportion'] = \\\n",
    "    X_train['tokens_no_sw'].apply(lambda x: len(''.join(x))) / X_train['text'].apply(len)\n",
    "\n",
    "# Average lemma length. Including stop words.\n",
    "meta_features_df_train['mean_lemma_length'] = \\\n",
    "    X_train['lemmas'].apply(lambda x: np.mean([len(l) for l in x]))\n",
    "\n",
    "# Lemma count. No stop words.\n",
    "meta_features_df_train['lemma_count_no_sw'] = \\\n",
    "    X_train['lemmas_no_sw'].apply(len)\n",
    "\n",
    "# Number of unique lemmas. No stop words.\n",
    "meta_features_df_train['unique_lemmas_no_sw'] = \\\n",
    "    X_train['lemmas_no_sw'].apply(lambda x: len(set(x)))\n",
    "\n",
    "# Proportion of stop words\n",
    "meta_features_df_train['sw_proportion'] = \\\n",
    "    X_train['tokens'].apply(lambda x: len([w for w in x if w in stopword_list]))\\\n",
    "        / X_train['tokens'].apply(len)\n",
    "\n",
    "# Proportion of hashtags\n",
    "meta_features_df_train['hashtag_proportion'] = \\\n",
    "    X_train['text'].apply(lambda x: len(re.findall(r'#{1}\\w+', x)))\\\n",
    "        / X_train['tokens'].apply(len)\n",
    "\n",
    "# Proportion of mentions\n",
    "meta_features_df_train['mention_proportion'] = \\\n",
    "    X_train['text'].apply(lambda x: len(re.findall(r'@{1}\\w+', x)))\\\n",
    "        / X_train['tokens'].apply(len)\n",
    "\n",
    "meta_features_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZING\n",
    "#\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_non = '#2c2fbf'\n",
    "color_disaster ='#f14848'\n",
    "kwargs_histplot = {'kde':True, 'stat':\"density\", 'linewidth':0, 'bins':'auto'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(15,10))\n",
    "fl_ax = axes.flatten()\n",
    "\n",
    "for idx, ftr in list(enumerate(meta_features_df_train.columns)):\n",
    "    sns.histplot(meta_features_df_train[ftr][y_train==0], ax=fl_ax[idx], **kwargs_histplot, color=color_non)\n",
    "    sns.histplot(meta_features_df_train[ftr][y_train==1], ax=fl_ax[idx], **kwargs_histplot, color=color_disaster)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# sns.countplot(meta_features_df_train['has_url'][y_train==0], ax=fl_ax[0], color=color_non)\n",
    "# sns.countplot(meta_features_df_train['has_url'][y_train==1], ax=fl_ax[0], color=color_disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END VISUALIZING\n",
    "#\n",
    "#\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "#  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the tweets\n",
    "\n",
    "I've engineered linguistic features and meta-features. I'm going to make the actual text of the tweets interpretable by a machine learning model. I'm using scikit-learn's TF-IDF vectorizer and the lemmatized tokens of the tweets.\n",
    "\n",
    "This vectorizor returns ______________. \n",
    "\n",
    "In the same step, I'm going to combine the vectors with the other features I've engineered so far into a single DataFrame. **This is the DataFrame that the model will train on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=spacy_lemmatize, \n",
    "    stop_words=stopword_list_lem,\n",
    "    max_features=500,\n",
    "    # binary=True\n",
    "    )\n",
    "X_train_vec = tfidf.fit_transform(X_train['text'])\n",
    "X_train_vec_df = pd.DataFrame(\n",
    "    X_train_vec.toarray(), columns=tfidf.get_feature_names(), index=X_train.index\n",
    "    )\n",
    "\n",
    "X_train_combined_df = pd.concat(\n",
    "    [\n",
    "        X_train_vec_df, \n",
    "        pos_vec_df_train, \n",
    "        ner_vec_df_train, \n",
    "        meta_features_df_train,\n",
    "    ],\n",
    "    axis=1\n",
    "    )\n",
    "\n",
    "# Scaling all features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_combined_df_scaled = scaler.fit_transform(X_train_combined_df)\n",
    "X_train_combined_df_scaled = pd.DataFrame(X_train_combined_df_scaled, index=X_train.index, columns=X_train_combined_df.columns)\n",
    "\n",
    "X_train_combined_df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply all pre-processing steps to test and validation sets.\n",
    "The function below runs the `test` and `val` sets through the exact same preprocessing steps that the `train` set as undergone.\n",
    "\n",
    "By default, the function makes use of the exact transformer objected that have been trained on `X_train`, in order to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(\n",
    "    df_to_process,\n",
    "    tokenizer=tokenizer,\n",
    "    stop_words=stopword_list, \n",
    "    lemmatizer=spacy_lemmatize,\n",
    "    stop_words_lem=stopword_list_lem,\n",
    "    pos_maker=spacy_pos,\n",
    "    trained_pos_vectorizer=pos_vectorizer,\n",
    "    trained_ner_vectorizer=ner_vectorizer,\n",
    "    trained_tfidf=tfidf,\n",
    "    trained_scaler=scaler,\n",
    "    return_scaled=True,\n",
    "    return_cleaned=False\n",
    "):\n",
    "    index=df_to_process.index\n",
    "    tweet_df = df_to_process.copy()\n",
    "\n",
    "    tweet_df['location'] = tweet_df['location'].fillna('location_missing')\n",
    "    \n",
    "    tweet_df['text'] = tweet_df['text'].apply(lambda x: x.lower())\n",
    "    has_url_Series = tweet_df['text'].apply(binary_url).rename('has_url')\n",
    "    tweet_df['has_url'] = has_url_Series\n",
    "    tweet_df['text'] = \\\n",
    "        tweet_df['text'].apply(lambda x: re.sub(pattern=r'http\\S+', repl='', string=x))\n",
    "\n",
    "    tweet_df['text'] = tweet_df['text'].apply(lambda x: re.sub(\"&amp;\", \"and\", x))\n",
    "    tweet_df['tokens'] = tweet_df['text'].apply(tokenizer.tokenize)\n",
    "    tweet_df['tokens_no_sw'] = \\\n",
    "        tweet_df['tokens'].apply(lambda x: [w for w in x if not w in stop_words])\n",
    "\n",
    "    tweet_df['lemmas'] = tweet_df['text'].apply(lemmatizer)\n",
    "    tweet_df['lemmas_no_sw'] = \\\n",
    "        tweet_df['lemmas'].apply(lambda x: [l for l in x if l not in stop_words_lem])\n",
    "\n",
    "    tweet_df['text_as_pos'] = tweet_df['text'].apply(pos_maker)\n",
    "    pos_vec = trained_pos_vectorizer.transform(tweet_df['text'])\n",
    "    pos_vec_df = pd.DataFrame(\n",
    "        pos_vec.toarray(), \n",
    "        columns=trained_pos_vectorizer.get_feature_names(),\n",
    "        index=index\n",
    "    )\n",
    "    pos_vec_df_norm = pos_vec_df.div(pos_vec_df.sum(axis=1), axis=0)\n",
    "    pos_vec_df_norm.columns = pos_vec_df.columns + '_norm'\n",
    "    interesting_tags = ['ADP_norm', 'ADV_norm', 'AUX_norm', 'NOUN_norm', 'PRON_norm', 'PROPN_norm']\n",
    "    pos_vec_df = pos_vec_df.join(pos_vec_df_norm[interesting_tags])\n",
    "\n",
    "    ner_vec = trained_ner_vectorizer.transform(tweet_df['text'])\n",
    "    ner_vec_df = pd.DataFrame(\n",
    "        ner_vec.toarray(),\n",
    "        columns=trained_ner_vectorizer.get_feature_names(),\n",
    "        index=index\n",
    "    )\n",
    "    ner_vec_df['location_GPE'] = tweet_df['location'].apply(lambda x: int('GPE' in [ent.label_ for ent in nlp(x).ents]))\n",
    "\n",
    "    meta_features_df = pd.DataFrame(has_url_Series)\n",
    "    # Character count. Original tweet. No URL.\n",
    "    meta_features_df['character_count'] = \\\n",
    "        tweet_df['text'].apply(len)\n",
    "    # Stop word count. Original tweet. No URL.\n",
    "    meta_features_df['sw_count'] = \\\n",
    "        tweet_df['tokens'].apply(lambda x: len([w for w in x if w in stop_words]))\n",
    "    # Character count of non-stop-words / original character count. No URL.\n",
    "    meta_features_df['non_sw_char_proportion'] = \\\n",
    "        tweet_df['tokens_no_sw'].apply(lambda x: len(''.join(x))) / tweet_df['text'].apply(len)\n",
    "    # Average lemma length. Including stop words.\n",
    "    meta_features_df['mean_lemma_length'] = \\\n",
    "        tweet_df['lemmas'].apply(lambda x: np.mean([len(l) for l in x]))\n",
    "    # Lemma count. No stop words.\n",
    "    meta_features_df['lemma_count_no_sw'] = \\\n",
    "        tweet_df['lemmas_no_sw'].apply(len)\n",
    "    # Number of unique lemmas. No stop words.\n",
    "    meta_features_df['unique_lemmas_no_sw'] = \\\n",
    "        tweet_df['lemmas_no_sw'].apply(lambda x: len(set(x)))\n",
    "    # Proportion of stop words\n",
    "    meta_features_df['sw_proportion'] = \\\n",
    "        tweet_df['tokens'].apply(lambda x: len([w for w in x if w in stop_words]))\\\n",
    "            / tweet_df['tokens'].apply(len)\\\n",
    "    # Proportion of hashtags\n",
    "    meta_features_df['hashtag_proportion'] = \\\n",
    "        tweet_df['text'].apply(lambda x: len(re.findall(r'#{1}\\w+', x)))\\\n",
    "            / tweet_df['tokens'].apply(len)\n",
    "    # Proportion of mentions\n",
    "    meta_features_df['mention_proportion'] = \\\n",
    "        tweet_df['text'].apply(lambda x: len(re.findall(r'@{1}\\w+', x)))\\\n",
    "            / tweet_df['tokens'].apply(len)\n",
    "\n",
    "    tweets_vec = trained_tfidf.transform(tweet_df['text'])\n",
    "    tweets_vec_df = pd.DataFrame(\n",
    "        tweets_vec.toarray(), columns=trained_tfidf.get_feature_names(), index=index\n",
    "    )\n",
    "    tweets_combined_df = pd.concat(\n",
    "        [\n",
    "            tweets_vec_df,\n",
    "            pos_vec_df,\n",
    "            ner_vec_df,\n",
    "            meta_features_df\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    tweets_combined_df_scaled = trained_scaler.transform(tweets_combined_df)\n",
    "    tweets_combined_df_scaled = pd.DataFrame(tweets_combined_df_scaled, index=index, columns=tweets_combined_df.columns)\n",
    "\n",
    "    if return_scaled:\n",
    "        if return_cleaned:\n",
    "            return {'processed':tweets_combined_df_scaled, 'cleaned':tweet_df}\n",
    "        else:\n",
    "            return tweets_combined_df_scaled\n",
    "    else:\n",
    "        if return_cleaned:\n",
    "            return {'processed':tweets_combined_df, 'cleaned':tweet_df}\n",
    "        else:\n",
    "            return tweets_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_combined_df_scaled = preprocess_tweets(X_test)\n",
    "X_val_combined_df_scaled = preprocess_tweets(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing NN\n",
    "\n",
    "def plot_nn_curves(model_history):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(18,12))\n",
    "    fl_ax = axes.flatten()\n",
    "    for idx, metric in enumerate(['loss', 'accuracy', 'precision', 'recall', 'auc', 'f1']):\n",
    "        pair = [m for m in model_history.history.keys() if metric in m]\n",
    "        fl_ax[idx].plot(model_history.history[pair[0]], label=metric)\n",
    "        fl_ax[idx].plot(model_history.history[pair[1]], label=metric+'_val')\n",
    "        fl_ax[idx].set_xlabel('epochs')\n",
    "        fl_ax[idx].set_ylabel(metric)\n",
    "        fl_ax[idx].set_title(f'{metric.upper()} Evaluation')\n",
    "        fl_ax[idx].legend()\n",
    "        plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined_df_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = X_train_combined_df_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making an f1 scorer for Keras\n",
    "## https://aakashgoel12.medium.com/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d#:~:text=By%20default%2C%20f1%20score%20is,like%20accuracy%2C%20categorical%20accuracy%20etc.\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "precision = Precision()\n",
    "recall = Recall()\n",
    "auc = AUC()\n",
    "f1 = get_f1 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layer=layers.Dense(\n",
    "        units=100,\n",
    "        activation='relu',\n",
    "        input_shape=(n_input,)\n",
    "))\n",
    "\n",
    "model.add(layer=layers.Dense(\n",
    "        units=100,\n",
    "        activation='relu'\n",
    "))\n",
    "\n",
    "model.add(layer=layers.Dense(\n",
    "        units=100,\n",
    "        activation='relu'\n",
    "))\n",
    "\n",
    "model.add(layer=layers.Dense(\n",
    "    units=1,\n",
    "    activation='sigmoid',\n",
    "))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='SGD',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', precision, recall, auc, f1]\n",
    ")\n",
    "\n",
    "model_val = model.fit(\n",
    "    np.array(X_train_combined_df_scaled),\n",
    "    np.array(y_train),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_combined_df_scaled, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nn_curves(model_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layer=layers.Dense(\n",
    "        units=200,\n",
    "        activation=LeakyReLU(0.005),\n",
    "        input_shape=(n_input,),\n",
    "        kernel_regularizer=regularizers.l2(0.005)\n",
    "    ))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layer=layers.Dense(\n",
    "    units=200,\n",
    "    activation=LeakyReLU(0.005),\n",
    "    kernel_regularizer=regularizers.l2(0.005)\n",
    "))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layer=layers.Dense(\n",
    "    units=200,\n",
    "    activation=LeakyReLU(0.005),\n",
    "    kernel_regularizer=regularizers.l2(0.005)\n",
    "))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layer=layers.Dense(\n",
    "    units=200,\n",
    "    activation=LeakyReLU(0.005),\n",
    "    kernel_regularizer=regularizers.l2(0.005)\n",
    "))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layer=layers.Dense(\n",
    "    units=200,\n",
    "    activation=LeakyReLU(0.005),\n",
    "    kernel_regularizer=regularizers.l2(0.005)\n",
    "))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layer=layers.Dense(\n",
    "    units=1,\n",
    "    activation='sigmoid',\n",
    "))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='SGD',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', precision, recall, auc, f1]\n",
    ")\n",
    "\n",
    "# early_stopping = [\n",
    "#     EarlyStopping(monitor='val_loss', patience=15)\n",
    "#     ]\n",
    "\n",
    "model_val = model.fit(\n",
    "    np.array(X_train_combined_df_scaled),\n",
    "    np.array(y_train),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_combined_df_scaled, y_val)\n",
    "    # callbacks=early_stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nn_curves(model_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import \\\n",
    "    accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, classification_report, plot_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "made_up_tweets = pd.read_csv('./stuff_to_ignore/made_up_tweets.csv')\n",
    "made_up_tweets_processed = preprocess_tweets(made_up_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = pd.Series(model.predict(made_up_tweets_processed).reshape(1,-1)[0]).rename('probs')\n",
    "preds = pd.Series(model.predict(made_up_tweets_processed).reshape(1,-1)[0]).apply(lambda x: int(x >= 0.5)).rename('preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(made_up_tweets['target'], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(made_up_tweets['target'], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(made_up_tweets['target'], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_preds = preprocess_tweets(made_up_tweets, return_cleaned=True, return_scaled=False)['cleaned'][['location', 'text', 'target']].join([preds, probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_preds['outcome'] = (with_preds['preds'] - with_preds['target']).map({-1:'NEGATIVE', 1:'POSITIVE', 0:'true'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_combined_df_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total time:\n",
    "t = time.time() - start_time\n",
    "print(f'Notebook run time: {t//60:.0f} minutes and {t%60:.0f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e508bc85c11e03fe10d0c37e7996d01ac3bac0c8b57b70908c892ac1dabf0c16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
